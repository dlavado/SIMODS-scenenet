\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lipton2018mythos}
\citation{guidotti2018survey}
\citation{doshi2017towards}
\citation{lipton2018mythos}
\citation{rudin2019stop}
\citation{Chefer_2021_CVPR}
\citation{voita2019analyzing}
\citation{ribeiro2016should}
\citation{fong2017interpretable}
\citation{rudin2019stop}
\citation{lipton2018mythos}
\citation{chen2020concept}
\citation{GENEO19}
\citation{GENEO21}
\citation{Semantic3D}
\citation{SemKITTI}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{figure.2}\protected@file@percent }
\citation{muhammad2020deep}
\citation{alzubaidi2021review}
\citation{thomas2019kpconv}
\citation{tang2020searching}
\citation{xu2021rpvnet}
\citation{yan2021sparse}
\citation{yan20222dpass}
\citation{SemKITTI}
\newlabel{fig:intro_input}{{1(a)}{2}{Subfigure 1(a)}{subfigure.3}{}}
\newlabel{fig:intro_input@cref}{{[subfigure][1][1]1(a)}{[1][1][]2}}
\newlabel{sub@fig:intro_input}{{(a)}{2}{Subfigure 1(a)\relax }{subfigure.3}{}}
\newlabel{fig:intro_gnet}{{1(b)}{2}{Subfigure 1(b)}{subfigure.4}{}}
\newlabel{fig:intro_gnet@cref}{{[subfigure][2][1]1(b)}{[1][1][]2}}
\newlabel{sub@fig:intro_gnet}{{(b)}{2}{Subfigure 1(b)\relax }{subfigure.4}{}}
\newlabel{fig:intro_cnn}{{1(c)}{2}{Subfigure 1(c)}{subfigure.5}{}}
\newlabel{fig:intro_cnn@cref}{{[subfigure][3][1]1(c)}{[1][1][]2}}
\newlabel{sub@fig:intro_cnn}{{(c)}{2}{Subfigure 1(c)\relax }{subfigure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Signature shapes for power line supporting tower detection. For our TS40K sample shown in (a), SCENE-Net accurately detects the body of the tower (b), while a comparable CNN has a large false positive area in the vegetation (c). Our model is interpretable with 11 trainable geometric parameters whereas the CNN has a total of 2190 parameters. The ground and power lines are mislabeled in the ground truth}}{2}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {TS40K Sample}}}{2}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {SCENE-Net}}}{2}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Baseline CNN}}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:intro_fig}{{1}{2}{Signature shapes for power line supporting tower detection. For our TS40K sample shown in (a), SCENE-Net accurately detects the body of the tower (b), while a comparable CNN has a large false positive area in the vegetation (c). Our model is interpretable with 11 trainable geometric parameters whereas the CNN has a total of 2190 parameters. The ground and power lines are mislabeled in the ground truth}{figure.2}{}}
\newlabel{fig:intro_fig@cref}{{[figure][1][]1}{[1][1][]2}}
\citation{long2015fully}
\citation{rethage2018fully}
\citation{tchapmi2017segcloud}
\citation{graham20183d}
\citation{su2018splatnet}
\citation{wang2017cnn}
\citation{le2018pointgrid}
\citation{qi2017pointnet}
\citation{qi2017pointnet++}
\citation{hu2020randla}
\citation{xu2020geometry}
\citation{hua2018pointwise}
\citation{li2018pointcnn}
\citation{wu2019pointconv}
\citation{thomas2019kpconv}
\citation{Cylinder3D}
\citation{SemKITTI}
\citation{SensatUrban}
\citation{AF2S3Net}
\citation{tang2020searching}
\citation{xu2021rpvnet}
\citation{yan20222dpass}
\citation{guo2020deep}
\citation{ding2021electric}
\citation{guo2019research}
\citation{tao2019study}
\newlabel{sec:related-work}{{2}{3}{Related Work}{section.8}{}}
\newlabel{sec:related-work@cref}{{[section][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Point Cloud Semantic Segmentation}{3}{subsection.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Power line segmentation from 3D point clouds.}{3}{subsection.10}\protected@file@percent }
\citation{lipton2018mythos}
\citation{guidotti2018survey}
\citation{doshi2017towards}
\citation{ribeiro2016should}
\citation{fong2017interpretable}
\citation{ribeiro2018anchors}
\citation{leite21}
\citation{rudin2019stop}
\citation{rudin2019stop}
\citation{rudin2019stop}
\citation{chen2020concept}
\citation{Zhang_2018_CVPR}
\citation{GENEO19}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Explainable Machine Learning}{4}{subsection.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Group Equivariant Non-Expansive Operators (GENEOs)}{4}{section.12}\protected@file@percent }
\citation{GENEO19}
\citation{bocchi2022geneonet}
\citation{conti2022construction}
\citation{botteghi2020finite}
\@writefile{thm}{\contentsline {definition}{{Definition}{3.1}{Group Equivariant Non-Expansive Operator (GENEO)}}{5}{theorem.13}\protected@file@percent }
\newlabel{sec:method}{{4}{5}{SCENE-Net: Signature geometriC Equivariant Non-Expansive operator Network}{section.16}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}SCENE-Net: Signature geometriC Equivariant Non-Expansive operator Network}{5}{section.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overview}{5}{subsection.19}\protected@file@percent }
\citation{GENEO19}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pipeline of SCENE-Net: an input point cloud $\mathcal  {P}$ is measured according to function $\varphi $ and voxelized. This representation then is fed to a GENEO-layer, where each operator $\Gamma _i^{\vartheta _i}$ separately convolves the input. A GENEO observer $\mathcal  {H}$ is then achieved by a convex combination of the operators in the GENEO layer. $\mathcal  {M}$ transforms the analysis of the observer into a probability of belonging to a tower. Lastly, a threshold operation is applied to classify the voxels. Note that this final step occurs after training is completed. }}{6}{figure.17}\protected@file@percent }
\newlabel{fig:gnet_overview}{{2}{6}{Pipeline of SCENE-Net: an input point cloud $\mathcal {P}$ is measured according to function $\varphi $ and voxelized. This representation then is fed to a GENEO-layer, where each operator $\Gamma _i^{\vartheta _i}$ separately convolves the input. A GENEO observer $\mathcal {H}$ is then achieved by a convex combination of the operators in the GENEO layer. $\mathcal {M}$ transforms the analysis of the observer into a probability of belonging to a tower. Lastly, a threshold operation is applied to classify the voxels. Note that this final step occurs after training is completed}{figure.17}{}}
\newlabel{fig:gnet_overview@cref}{{[figure][2][]2}{[1][5][]6}}
\newlabel{eq:observer}{{4.1}{6}{Overview}{equation.20}{}}
\newlabel{eq:observer@cref}{{[equation][1][4]4.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Knowledge Engineering via GENEOs}{7}{subsection.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Cylinder GENEO}{7}{subsubsection.22}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{4.1}{}}{7}{theorem.23}\protected@file@percent }
\newlabel{prop:cylinder}{{4.1}{7}{Cylinder GENEO}{theorem.23}{}}
\newlabel{prop:cylinder@cref}{{[definition][1][4]4.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Arrow GENEO}{8}{subsubsection.24}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{4.2}{}}{8}{theorem.25}\protected@file@percent }
\newlabel{prop:arrow}{{4.2}{8}{Arrow GENEO}{theorem.25}{}}
\newlabel{prop:arrow@cref}{{[definition][2][4]4.2}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Negative Sphere GENEO}{8}{subsubsection.26}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{4.3}{}}{8}{theorem.27}\protected@file@percent }
\newlabel{prop:ns}{{4.3}{8}{Negative Sphere GENEO}{theorem.27}{}}
\newlabel{prop:ns@cref}{{[definition][3][4]4.3}{[1][8][]8}}
\citation{steininger2021density}
\newlabel{fig:cylinder}{{3(a)}{9}{Subfigure 3(a)}{subfigure.29}{}}
\newlabel{fig:cylinder@cref}{{[subfigure][1][3]3(a)}{[1][9][]9}}
\newlabel{sub@fig:cylinder}{{(a)}{9}{Subfigure 3(a)\relax }{subfigure.29}{}}
\newlabel{fig:arrow}{{3(b)}{9}{Subfigure 3(b)}{subfigure.30}{}}
\newlabel{fig:arrow@cref}{{[subfigure][2][3]3(b)}{[1][9][]9}}
\newlabel{sub@fig:arrow}{{(b)}{9}{Subfigure 3(b)\relax }{subfigure.30}{}}
\newlabel{fig:neg_sphere}{{3(c)}{9}{Subfigure 3(c)}{subfigure.31}{}}
\newlabel{fig:neg_sphere@cref}{{[subfigure][3][3]3(c)}{[1][9][]9}}
\newlabel{sub@fig:neg_sphere}{{(c)}{9}{Subfigure 3(c)\relax }{subfigure.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces GENEO kernels discretized in a voxel grid and colored according to weight distribution}}{9}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Cylinder}}}{9}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Arrow}}}{9}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Negative Sphere}}}{9}{figure.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}GENEO Loss}{9}{subsection.34}\protected@file@percent }
\newlabel{eq:opt2}{{4.2}{10}{GENEO Loss}{equation.35}{}}
\newlabel{eq:opt2@cref}{{[equation][2][4]4.2}{[1][10][]10}}
\newlabel{eq:opt_final}{{4.3}{10}{GENEO Loss}{equation.36}{}}
\newlabel{eq:opt_final@cref}{{[equation][3][4]4.3}{[1][10][]10}}
\newlabel{sec:experiments}{{5}{10}{Experiments}{section.37}{}}
\newlabel{sec:experiments@cref}{{[section][5][]5}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{10}{section.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset}{10}{figure.42}\protected@file@percent }
\citation{thomas2019kpconv}
\citation{AF2S3Net}
\citation{xu2021rpvnet}
\citation{yan20222dpass}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Crop sample from the TS40K dataset.}}{11}{figure.39}\protected@file@percent }
\newlabel{fig:crop}{{4}{11}{Crop sample from the TS40K dataset}{figure.39}{}}
\newlabel{fig:crop@cref}{{[figure][4][]4}{[1][10][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Voxelization of sample in Figure\nobreakspace  {}\ref  {fig:crop}.}}{11}{figure.42}\protected@file@percent }
\newlabel{fig:vox}{{5}{11}{Voxelization of sample in Figure~\ref {fig:crop}}{figure.42}{}}
\newlabel{fig:vox@cref}{{[figure][5][]5}{[1][10][]11}}
\newlabel{sec:train_protocol}{{5.2}{11}{Training Protocol}{subsection.43}{}}
\newlabel{sec:train_protocol@cref}{{[subsection][2][5]5.2}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training Protocol}{11}{subsection.43}\protected@file@percent }
\newlabel{sec:results-interpretability}{{5.3}{11}{Interpretability of the trained SCENE-Net: The meaning of the 11 learned parameters}{subsection.44}{}}
\newlabel{sec:results-interpretability@cref}{{[subsection][3][5]5.3}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Interpretability of the trained SCENE-Net: The meaning of the 11 learned parameters}{11}{subsection.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Post-hoc interpretation for specific predictions}{11}{subsection.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The trainable parameters of SCENE-Net, $\vartheta $ and $\lambda $. Parameter $h^*$ is not trainable, and $\lambda _{NS}^*$ is defined as a function of the other mixing weights $\lambda _{NS}^* = 1 - \lambda _{Ar} - \lambda _{Cy}$}}{12}{figure.45}\protected@file@percent }
\newlabel{fig:interpretable}{{6}{12}{The trainable parameters of SCENE-Net, $\vartheta $ and $\lambda $. Parameter $h^*$ is not trainable, and $\lambda _{NS}^*$ is defined as a function of the other mixing weights $\lambda _{NS}^* = 1 - \lambda _{Ar} - \lambda _{Cy}$}{figure.45}{}}
\newlabel{fig:interpretable@cref}{{[figure][6][]6}{[1][11][]12}}
\newlabel{sec:results-accuracy}{{5.5}{12}{Qualitative accuracy and quantitative metrics: SCENE-Net is more precise in detecting towers than a baseline CNN}{subsection.55}{}}
\newlabel{sec:results-accuracy@cref}{{[subsection][5][5]5.5}{[1][11][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Qualitative accuracy and quantitative metrics: SCENE-Net is more precise in detecting towers than a baseline CNN}{12}{subsection.55}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 3D semantic segmentation metrics on TS40K}}{12}{table.56}\protected@file@percent }
\newlabel{tab:res_GENEO}{{1}{12}{3D semantic segmentation metrics on TS40K}{table.56}{}}
\newlabel{tab:res_GENEO@cref}{{[table][1][]1}{[1][12][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textit  {Post hoc} analysis of SCENE-Net. We can examine the activation of each geometric operator and correlate it to the detection of certain elements in the scene. We see that the Arrow is responsible for the most activation, while the Negative Sphere has a smaller absolute value}}{13}{figure.48}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {TS40K scene}}}{13}{figure.48}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Cylinder}}}{13}{figure.48}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Arrow}}}{13}{figure.48}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Negative Sphere}}}{13}{figure.48}\protected@file@percent }
\newlabel{fig:posthoc}{{7}{13}{\textit {Post hoc} analysis of SCENE-Net. We can examine the activation of each geometric operator and correlate it to the detection of certain elements in the scene. We see that the Arrow is responsible for the most activation, while the Negative Sphere has a smaller absolute value}{figure.48}{}}
\newlabel{fig:posthoc@cref}{{[figure][7][]7}{[1][11][]13}}
\newlabel{sec:results-robust}{{5.6}{13}{SCENE-Net is robust to noisy labels}{subsection.60}{}}
\newlabel{sec:results-robust@cref}{{[subsection][6][5]5.6}{[1][12][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}SCENE-Net is robust to noisy labels}{13}{subsection.60}\protected@file@percent }
\citation{salehi2017tversky}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Precision-Recall curve for SCENE-Net and the CNN benchmark, with changing detection threshold. Although our model SCENE-Net has two orders of magnitude fewer parameters than the CNN, it attains a comparable area under the P-R curve}}{14}{figure.58}\protected@file@percent }
\newlabel{fig:recall}{{8}{14}{Precision-Recall curve for SCENE-Net and the CNN benchmark, with changing detection threshold. Although our model SCENE-Net has two orders of magnitude fewer parameters than the CNN, it attains a comparable area under the P-R curve}{figure.58}{}}
\newlabel{fig:recall@cref}{{[figure][8][]8}{[1][12][]14}}
\newlabel{sec:results-time-efficiency}{{5.7}{14}{SCENE-Net has low data requirements and has modest training time in common hardware}{subsection.66}{}}
\newlabel{sec:results-time-efficiency@cref}{{[subsection][7][5]5.7}{[1][13][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}SCENE-Net has low data requirements and has modest training time in common hardware}{14}{subsection.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}SCENE-Net on the SemanticKITTI: an efficient model for low-resource contexts}{14}{subsection.67}\protected@file@percent }
\citation{qi2017pointnet++}
\citation{tatarchenko2018tangent}
\citation{thomas2019kpconv}
\citation{hu2020randla}
\citation{xu2021rpvnet}
\citation{graham20183d}
\citation{yan2021sparse}
\citation{tang2020searching}
\newlabel{fig:robust_input}{{9(a)}{15}{Subfigure 9(a)}{subfigure.62}{}}
\newlabel{fig:robust_input@cref}{{[subfigure][1][9]9(a)}{[1][12][]15}}
\newlabel{sub@fig:robust_input}{{(a)}{15}{Subfigure 9(a)\relax }{subfigure.62}{}}
\newlabel{fig:robust_pred}{{9(b)}{15}{Subfigure 9(b)}{subfigure.63}{}}
\newlabel{fig:robust_pred@cref}{{[subfigure][2][9]9(b)}{[1][12][]15}}
\newlabel{sub@fig:robust_pred}{{(b)}{15}{Subfigure 9(b)\relax }{subfigure.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces SCENE-Net is robust against mislabeled data. Fig.\nobreakspace  {}\ref  {fig:robust_pred} compares the prediction of SCENE-Net against the ground truth in Fig.\nobreakspace  {}\ref  {fig:robust_input}. SCENE-Net detects the body of the tower, ignoring the patch of ground mislabeled as a tower}}{15}{figure.61}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {TS40K scene}}}{15}{figure.61}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {SCENE-Net prediction against the GT}}}{15}{figure.61}\protected@file@percent }
\newlabel{fig:robustness}{{9}{15}{SCENE-Net is robust against mislabeled data. Fig.~\ref {fig:robust_pred} compares the prediction of SCENE-Net against the ground truth in Fig.~\ref {fig:robust_input}. SCENE-Net detects the body of the tower, ignoring the patch of ground mislabeled as a tower}{figure.61}{}}
\newlabel{fig:robustness@cref}{{[figure][9][]9}{[1][12][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Ablation Studies}{15}{table.78}\protected@file@percent }
\newlabel{sec:results-resolution}{{5.10}{15}{SCENE-Net inference in high resolution, when trained with low-resolution kernel sizes}{subsection.80}{}}
\newlabel{sec:results-resolution@cref}{{[subsection][10][5]5.10}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}SCENE-Net inference in high resolution, when trained with low-resolution kernel sizes}{15}{subsection.80}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Semantic segmentation on \textit  {SemanticKITTI}, only methods that report the parameter count were included. Large models are included for comparison but cannot be used in low-resource contexts. Parameter efficiency is $\frac  {\textrm  {Pole IoU}}{\textrm  {log \#Parameters}}$}}{16}{table.68}\protected@file@percent }
\newlabel{tab:sota_perf}{{2}{16}{Semantic segmentation on \textit {SemanticKITTI}, only methods that report the parameter count were included. Large models are included for comparison but cannot be used in low-resource contexts. Parameter efficiency is $\frac {\textrm {Pole IoU}}{\textrm {log \#Parameters}}$}{table.68}{}}
\newlabel{tab:sota_perf@cref}{{[table][2][]2}{[1][15][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Semantic Segmentation results on the \textit  {SemanticKITTI} benchmark. Log scale is used for more intelligible comparison}}{16}{figure.70}\protected@file@percent }
\newlabel{fig:semK_perf}{{10}{16}{Semantic Segmentation results on the \textit {SemanticKITTI} benchmark. Log scale is used for more intelligible comparison}{figure.70}{}}
\newlabel{fig:semK_perf@cref}{{[figure][10][]10}{[1][15][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.11}Template Matching Comparison}{16}{subsection.86}\protected@file@percent }
\citation{yan2021sparse}
\citation{xu2021rpvnet}
\newlabel{fig:semK_input}{{11(a)}{17}{Subfigure 11(a)}{subfigure.73}{}}
\newlabel{fig:semK_input@cref}{{[subfigure][1][11]11(a)}{[1][15][]17}}
\newlabel{sub@fig:semK_input}{{(a)}{17}{Subfigure 11(a)\relax }{subfigure.73}{}}
\newlabel{fig:semK_pred}{{11(b)}{17}{Subfigure 11(b)}{subfigure.74}{}}
\newlabel{fig:semK_pred@cref}{{[subfigure][2][11]11(b)}{[1][15][]17}}
\newlabel{sub@fig:semK_pred}{{(b)}{17}{Subfigure 11(b)\relax }{subfigure.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Qualitative results of SCENE-Net on \textit  {SemanticKITTI} for pole detection. Fig.\nobreakspace  {}\ref  {fig:semK_pred} compares the prediction of SCENE-Net against the ground truth in Fig.\nobreakspace  {}\ref  {fig:semK_input}. SCENE-Net detects the body of the pole while disregarding the rest of the 3D scene}}{17}{figure.72}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\textit {SemanticKITTI} scene}}}{17}{figure.72}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {SCENE-Net prediction against the GT}}}{17}{figure.72}\protected@file@percent }
\newlabel{fig:semK_qualitative}{{11}{17}{Qualitative results of SCENE-Net on \textit {SemanticKITTI} for pole detection. Fig.~\ref {fig:semK_pred} compares the prediction of SCENE-Net against the ground truth in Fig.~\ref {fig:semK_input}. SCENE-Net detects the body of the pole while disregarding the rest of the 3D scene}{figure.72}{}}
\newlabel{fig:semK_qualitative@cref}{{[figure][11][]11}{[1][15][]17}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Ablation Study of SCENE-Net on TS40K validation set.}}{17}{table.78}\protected@file@percent }
\newlabel{tab:ablation}{{3}{17}{Ablation Study of SCENE-Net on TS40K validation set}{table.78}{}}
\newlabel{tab:ablation@cref}{{[table][3][]3}{[1][15][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{17}{section.87}\protected@file@percent }
\bibstyle{siamplain}
\bibdata{references}
\bibcite{alzubaidi2021review}{1}
\bibcite{SemKITTI}{2}
\newlabel{fig:128_input}{{12(a)}{18}{Subfigure 12(a)}{subfigure.82}{}}
\newlabel{fig:128_input@cref}{{[subfigure][1][12]12(a)}{[1][15][]18}}
\newlabel{sub@fig:128_input}{{(a)}{18}{Subfigure 12(a)\relax }{subfigure.82}{}}
\newlabel{fig:128_pred}{{12(b)}{18}{Subfigure 12(b)}{subfigure.83}{}}
\newlabel{fig:128_pred@cref}{{[subfigure][2][12]12(b)}{[1][15][]18}}
\newlabel{sub@fig:128_pred}{{(b)}{18}{Subfigure 12(b)\relax }{subfigure.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SCENE-Net is independent of the input and kernel size. Our model was trained with voxel grids of shape $64^3$ and kernel size $9^3$. Fig.\nobreakspace  {}\ref  {fig:128_pred} shows SCENE-Net's prediction against the ground truth of the $128^3$ input grid in Fig.\nobreakspace  {}\ref  {fig:128_input} using a kernel-size of $12\times 5\times 5$. }}{18}{figure.81}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Sample discretized in a $128^3$ grid}}}{18}{figure.81}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {SCENE-Net prediction against the GT}}}{18}{figure.81}\protected@file@percent }
\newlabel{fig:vxg128}{{12}{18}{SCENE-Net is independent of the input and kernel size. Our model was trained with voxel grids of shape $64^3$ and kernel size $9^3$. Fig.~\ref {fig:128_pred} shows SCENE-Net's prediction against the ground truth of the $128^3$ input grid in Fig.~\ref {fig:128_input} using a kernel-size of $12\times 5\times 5$}{figure.81}{}}
\newlabel{fig:vxg128@cref}{{[figure][12][]12}{[1][15][]18}}
\bibcite{GENEO19}{3}
\bibcite{bocchi2022geneonet}{4}
\bibcite{botteghi2020finite}{5}
\bibcite{GENEO21}{6}
\bibcite{Chefer_2021_CVPR}{7}
\bibcite{chen2020concept}{8}
\bibcite{AF2S3Net}{9}
\bibcite{conti2022construction}{10}
\bibcite{ding2021electric}{11}
\bibcite{doshi2017towards}{12}
\bibcite{fong2017interpretable}{13}
\bibcite{graham20183d}{14}
\bibcite{guidotti2018survey}{15}
\bibcite{guo2019research}{16}
\bibcite{guo2020deep}{17}
\bibcite{Semantic3D}{18}
\bibcite{SensatUrban}{19}
\bibcite{hu2020randla}{20}
\bibcite{hua2018pointwise}{21}
\bibcite{le2018pointgrid}{22}
\bibcite{li2018pointcnn}{23}
\bibcite{lipton2018mythos}{24}
\bibcite{long2015fully}{25}
\bibcite{muhammad2020deep}{26}
\bibcite{qi2017pointnet}{27}
\bibcite{qi2017pointnet++}{28}
\bibcite{rethage2018fully}{29}
\bibcite{leite21}{30}
\bibcite{ribeiro2016should}{31}
\bibcite{ribeiro2018anchors}{32}
\bibcite{rudin2019stop}{33}
\bibcite{salehi2017tversky}{34}
\bibcite{steininger2021density}{35}
\bibcite{su2018splatnet}{36}
\bibcite{tang2020searching}{37}
\bibcite{tao2019study}{38}
\bibcite{tatarchenko2018tangent}{39}
\bibcite{tchapmi2017segcloud}{40}
\bibcite{thomas2019kpconv}{41}
\bibcite{voita2019analyzing}{42}
\bibcite{wang2017cnn}{43}
\bibcite{wu2019pointconv}{44}
\bibcite{xu2021rpvnet}{45}
\bibcite{xu2020geometry}{46}
\bibcite{yan2021sparse}{47}
\bibcite{yan20222dpass}{48}
\bibcite{Zhang_2018_CVPR}{49}
\bibcite{Cylinder3D}{50}
\gdef \@abspage@last{21}
